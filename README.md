# AI Hiring Bias Analysis

![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=plotly&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)
![Fairlearn](https://img.shields.io/badge/Fairlearn-2E8B57?style=for-the-badge&logo=python&logoColor=white)


**Authors**

[![Static Badge](https://img.shields.io/badge/Xiyuan%20Wu-path?style=for-the-badge&color=%2387CEEB)](https://github.com/XiyuanWu)
[![Static Badge](https://img.shields.io/badge/Daniel%20Maciel-path?style=for-the-badge&color=%2390EE90)]()
[![Static Badge](https://img.shields.io/badge/Byron%20Bhuiyan-path?style=for-the-badge&color=%23CBC3E3)]()

## Overview

Hiring decisions are increasingly supported by AI-driven systems designed to screen and rank candidates efficiently. While these systems promise consistency and scalability, they also risk learning and amplifying historical biases embedded in recruitment data. When sensitive attributes such as gender or age are correlated—directly or indirectly—with hiring outcomes, automated models may unintentionally disadvantage certain groups, raising serious ethical and legal concerns.

This project focuses on examining potential bias within an AI-supported hiring context using a real-world recruitment dataset. By modeling the hiring decision and analyzing how outcomes vary across demographic groups, we aim to better understand how bias can arise in data-driven systems and why fairness evaluation is essential when deploying machine learning in high-stakes domains like employment.
